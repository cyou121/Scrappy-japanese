{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "\n",
    "class LMconfig():\n",
    "    def __init__(self)-> None:\n",
    "        dim: int = 512,  # 模型维度，默认为 512\n",
    "        n_layers: int = 5,  # Transformer 层数，默认为 8\n",
    "        n_heads: int = 16,  # 注意力头数，默认为 16\n",
    "        \n",
    "        vocab_size: int = 7000,  # 词汇表大小，默认为 6400\n",
    "        eps: float = 1e-5,  # 归一化层的 epsilon 值，默认为 1e-5\n",
    "        max_seq_len: int = 512,  # 最大序列长度，默认为 512\n",
    "        dropout: float = 0.1,  # Dropout 概率，默认为 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 定义 precompute_pos_cis 函数，用于预计算位置编码的复数形式\n",
    "def precompute_pos_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))  # 计算频率\n",
    "    t = torch.arange(end, device=freqs.device)  # 生成时间序列\n",
    "    freqs = torch.outer(t, freqs).float()  # 计算外积\n",
    "    pos_cis = torch.polar(torch.ones_like(freqs), freqs)  # 计算复数形式的位置编码\n",
    "    return pos_cis\n",
    "\n",
    "# 定义 apply_rotary_emb 函数，用于应用旋转位置编码\n",
    "def apply_rotary_emb(xq, xk, pos_cis):\n",
    "    def unite_shape(pos_cis, x):\n",
    "        ndim = x.ndim\n",
    "        assert 0 <= 1 < ndim\n",
    "        assert pos_cis.shape == (x.shape[1], x.shape[-1])\n",
    "        shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "        return pos_cis.view(*shape)\n",
    "\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))  # 将 xq 转换为复数形式\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))  # 将 xk 转换为复数形式\n",
    "    pos_cis = unite_shape(pos_cis, xq_)  # 调整 pos_cis 的形状\n",
    "    xq_out = torch.view_as_real(xq_ * pos_cis).flatten(3)  # 应用旋转位置编码\n",
    "    xk_out = torch.view_as_real(xk_ * pos_cis).flatten(3)  # 应用旋转位置编码\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)  # 返回结果\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RMS(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps  # 设置 epsilon，防止除零错误\n",
    "        self.weight = nn.Parameter(torch.ones(dim))  # 初始化权重参数\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)  # 计算 RMSNorm\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)  # 应用 RMSNorm\n",
    "        return output * self.weight  # 乘以权重参数\n",
    "\n",
    "class MutiHeadattention(nn.Module):\n",
    "    def __init__(self)->None:\n",
    "        super().__init__()\n",
    "        params=LMconfig()\n",
    "        self.hidden_dim=params.dim\n",
    "        self.head_num=params.head_num\n",
    "        self.head_dim=self.hidden_dim//self.head_num\n",
    "        self.max_seq_len=params.max_seq_len\n",
    "\n",
    "        self.q_proj=nn.Linear(self.head_dim,self.hidden_dim)\n",
    "        self.k_proj=nn.Linear(self.head_dim,self.hidden_dim)\n",
    "        self.v_proj=nn.Linear(self.head_dim,self.hidden_dim)\n",
    "        \n",
    "        self.att_dropout=nn.Dropout(0.1)\n",
    "        self.out_proj=nn.Linear(hidden_dim,self.head_dim)\n",
    "        \n",
    "        mask = torch.full((self.max_seq_len, self.max_seq_len), float(\"-inf\"))  \n",
    "        mask = torch.triu(mask, diagonal=1)  \n",
    "        self.k_cache=None\n",
    "        self.v_cache=None\n",
    "        self.use_kvcache=None\n",
    "        self.register_buffer(\"mask\", mask)  \n",
    "\n",
    "    def forward(self,x:torch.Tensor,pos_cis:torch.Tensor):\n",
    "        batch_size,seq_len,_ =x.shape\n",
    "\n",
    "        if self.use_kvcache and self.eval():\n",
    "            if self.k_cache and self.v_cache is not None:\n",
    "                token=x[:,-1,:]\n",
    "                q=torch.cat(torch.zeros_like(x[:, :-1, :]),self.q_proj(token),dim=1)\n",
    "                k=torch.cat(self.k_cache,self.k_proj(token),dim=1)\n",
    "                v=torch.cat(self.v_cache,self.v_proj(token),dim=1)\n",
    "            else:\n",
    "                q=self.q_proj(x)\n",
    "                k=self.k_proj(x)\n",
    "                v=self.v_proj(x)\n",
    "            self.k_cache=k\n",
    "            self.v_cache=v\n",
    "        else:\n",
    "            q=self.q_proj(x)\n",
    "            k=self.k_proj(x)\n",
    "            v=self.v_proj(x)\n",
    "\n",
    "\n",
    "        print(\"1\")\n",
    "        # q: [batch_size,seq_len,hidden_dim]\n",
    "        # k: [batch_size,seq_len,hidden_dim]\n",
    "        # v: [batch_size,seq_len,hidden_dim]\n",
    "        \n",
    "        q=q.view(batch_size,seq_len,self.head_num,self.head_dim)\n",
    "        k=k.view(batch_size,seq_len,self.head_num,self.head_dim)\n",
    "        v=v.view(batch_size,seq_len,self.head_num,self.head_dim)\n",
    "        print(\"2\")\n",
    "        \n",
    "        q, k = apply_rotary_emb(q, k, pos_cis)  # 应用旋转位置编码\n",
    "\n",
    "        q = q.transpose(1, 2)  # 调整 Q 的形状\n",
    "        k = k.transpose(1, 2)  # 调整 K 的形状\n",
    "        v = v.transpose(1, 2)  # 调整 V 的形状\n",
    "\n",
    "\n",
    "\n",
    "        # q: [batch_size,elf.head_num,seq_len,s,self.head_dim]\n",
    "        # k: [batch_size,elf.head_num,seq_len,s,self.head_dim]\n",
    "        # v: [batch_size,elf.head_num,seq_len,s,self.head_dim]\n",
    "\n",
    "        atten_weight=q@k.transpose(-2,-1)/math.sqrt(self.head_dim)\n",
    "        atten_weight+=self.mask[:seq_len,:seq_len]\n",
    "        \n",
    "        atten_weight=torch.softmax(atten_weight,dim=-1)\n",
    "        print(atten_weight)\n",
    "        atten_weight=self.att_dropout(atten_weight)\n",
    "        attention_output=atten_weight@v\n",
    "\n",
    "        attention_output=attention_output.transpose(1,2).contiguous()\n",
    "\n",
    "        attention_output=attention_output.view(batch_size,seq_len,self.hidden_dim)\n",
    "        return self.out_proj(attention_output)\n",
    "\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self)-> None:\n",
    "     super().__init__()\n",
    "     params=LMconfig()\n",
    "     self.Linear=nn.Linear(params.dim,params.dim,bias=False)\n",
    "     self.RMS=RMS(params.dim,eps=params.eps)\n",
    "     self.dropout=nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        start=x\n",
    "        x=self.RMS(x)\n",
    "        x1 = self.Linear(x)\n",
    "        x2 = self.Linear(x)\n",
    "        x2=F.silu(x2)\n",
    "        x3=torch.matmul(x1,x2)\n",
    "        x3=self.Linear(x3)\n",
    "        x3=self.dropout(x3)\n",
    "        x3+=start\n",
    "\n",
    "        return x3\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self)-> None:\n",
    "     super().__init__()\n",
    "     params=LMconfig()\n",
    "     self.MutiHeadattention=MutiHeadattention()\n",
    "     self.FFN=FFN()\n",
    "     self.attention_norm=RMS(params.dim,eps=params.eps)\n",
    "     self.FFN_norm=RMS(params.dim,eps=params.eps)\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor,pos_cis)->torch.Tensor:\n",
    "       x1= self.MutiHeadattention(self.attention_norm(x),pos_cis)+x\n",
    "       x2= self.FFN(self.FFN_norm(x1))+x1\n",
    "       return x2\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self)-> None:\n",
    "        super().__init__()\n",
    "        params=LMconfig()\n",
    "        self.TransformerBlock=TransformerBlock()\n",
    "        self.decoder_layers=nn.ModuleList([self.TransformerBlock for i in range(params.n_layers)])\n",
    "        self.rmsnorm=RMS(params.dim,params.eps)##1\n",
    "        self.dropout=nn.Dropout(params.dropout)\n",
    "        self.embedding=nn.Embedding(params.vocab_size,params.dim)\n",
    "\n",
    "\n",
    "        \n",
    "        pos_cis = precompute_pos_cis(self.params.dim // self.params.head_num, self.params.max_seq_len)  # 预计算位置编码\n",
    "        self.register_buffer(\"pos_cis\", pos_cis, persistent=False)  # 注册位置编码缓冲区\n",
    "\n",
    "\n",
    "        self.Linear=nn.Linear(params.dim,params.vocab_size,bias=False)\n",
    "        self.OUT = CausalLMOutputWithPast()  \n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, targets: torch.Tensor,**keyargs) ->torch.Tensor:\n",
    "        # if 'input_ids' in keyargs:\n",
    "        #     token=keyargs['input_ids']\n",
    "        print(\"tokens:\",tokens)\n",
    "        print(\"targets:\",targets)\n",
    "        try:\n",
    "         _bszize, seqlen,_ = tokens.shape\n",
    "        except ValueError as e:\n",
    "          print(\"训练时，输入的Size有错,训练中断\")\n",
    "        \n",
    "            \n",
    "        token=self.embedding(token)\n",
    "        token=self.dropout(token)\n",
    "\n",
    "        pos_cis = self.pos_cis[:seqlen]\n",
    "\n",
    "\n",
    "        for _,decoder_layers in enumerate(self.decoder_layers):\n",
    "            token=decoder_layers(token,pos_cis)\n",
    "        token=self.rmsnorm(token)\n",
    "        logits=self.Linear(token)\n",
    "        # token=nn.Softmax(token,dim=1)\n",
    "\n",
    "        print(\"合并batch 一次性计算交叉熵损失，的矩阵大小为：\",logits.view(-1, logits.size(-1)).shape)\n",
    "        self.last_loss=F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)  # 计算交叉熵损失\n",
    "\n",
    "        self.OUT.__setitem__('logits', logits)  # 设置输出对象的 logits\n",
    "        self.OUT.__setitem__('last_loss', self.last_loss)  # 设置输出对象的 last_loss\n",
    "        \n",
    "\n",
    "        return self.OUT\n",
    "    \n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate(self,token):\n",
    "\n",
    "        token=nn.Embedding(token)\n",
    "        for item,decoder_layers in enumerate(self.decoder_layers):\n",
    "            token=decoder_layers(token)\n",
    "        token=self.rmsnorm(token)\n",
    "        logits=self.Linear(token)\n",
    "\n",
    "\n",
    "        return token\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 precompute_pos_cis 函数，用于预计算位置编码的复数形式\n",
    "def precompute_pos_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))  # 计算频率\n",
    "    t = torch.arange(end, device=freqs.device)  # 生成时间序列\n",
    "    freqs = torch.outer(t, freqs).float()  # 计算外积\n",
    "    pos_cis = torch.polar(torch.ones_like(freqs), freqs)  # 计算复数形式的位置编码\n",
    "    return pos_cis\n",
    "\n",
    "# 定义 apply_rotary_emb 函数，用于应用旋转位置编码\n",
    "def apply_rotary_emb(xq, xk, pos_cis):\n",
    "    def unite_shape(pos_cis, x):\n",
    "        ndim = x.ndim\n",
    "        assert 0 <= 1 < ndim\n",
    "        assert pos_cis.shape == (x.shape[1], x.shape[-1])\n",
    "        shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "        return pos_cis.view(*shape)\n",
    "\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))  # 将 xq 转换为复数形式\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))  # 将 xk 转换为复数形式\n",
    "    pos_cis = unite_shape(pos_cis, xq_)  # 调整 pos_cis 的形状\n",
    "    xq_out = torch.view_as_real(xq_ * pos_cis).flatten(3)  # 应用旋转位置编码\n",
    "    xk_out = torch.view_as_real(xk_ * pos_cis).flatten(3)  # 应用旋转位置编码\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)  # 返回结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LMconfig():\n",
    "        dim: int = 512  # 模型维度，默认为 512\n",
    "        n_layers: int = 5  # Transformer 层数，默认为 8\n",
    "        head_num: int = 16  # 注意力头数，默认为 16\n",
    "        \n",
    "        vocab_size: int = 7000 # 词汇表大小，默认为 6400\n",
    "        eps: float = 1e-5 # 归一化层的 epsilon 值，默认为 1e-5\n",
    "        max_seq_len: int = 512# 最大序列长度，默认为 512\n",
    "        dropout: float = 0.1 # Dropout 概率，默认为 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiHeadattention(nn.Module):\n",
    "    def __init__(self)->None:\n",
    "        super().__init__()\n",
    "        params=LMconfig()\n",
    "        self.hidden_dim=params.dim\n",
    "        self.head_num=params.head_num\n",
    "        self.head_dim=self.hidden_dim//self.head_num\n",
    "        self.max_seq_len=params.max_seq_len\n",
    "\n",
    "        self.q_proj=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.k_proj=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.v_proj=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        \n",
    "        self.att_dropout=nn.Dropout(0.1)\n",
    "        self.out_proj=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        \n",
    "        mask = torch.full((self.max_seq_len, self.max_seq_len), float(\"-inf\"))  \n",
    "        mask = torch.triu(mask, diagonal=1)  \n",
    "        self.k_cache=None\n",
    "        self.v_cache=None\n",
    "        self.use_kvcache=None\n",
    "        self.register_buffer(\"mask\", mask)  \n",
    "\n",
    "    def forward(self,x:torch.Tensor,pos_cis:torch.Tensor):\n",
    "        batch_size,seq_len,_ =x.shape\n",
    "\n",
    "        if self.use_kvcache and self.eval():\n",
    "            if self.k_cache and self.v_cache is not None:\n",
    "                token=x[:,-1,:]\n",
    "                q=torch.cat(torch.zeros_like(x[:, :-1, :]),self.q_proj(token),dim=1)\n",
    "                k=torch.cat(self.k_cache,self.k_proj(token),dim=1)\n",
    "                v=torch.cat(self.v_cache,self.v_proj(token),dim=1)\n",
    "            else:\n",
    "                q=self.q_proj(x)\n",
    "                k=self.k_proj(x)\n",
    "                v=self.v_proj(x)\n",
    "            self.k_cache=k\n",
    "            self.v_cache=v\n",
    "        else:\n",
    "            q=self.q_proj(x)\n",
    "            k=self.k_proj(x)\n",
    "            v=self.v_proj(x)\n",
    "\n",
    "        print(q.shape)\n",
    "        print(\"1\")\n",
    "        # q: [batch_size,seq_len,hidden_dim]\n",
    "        # k: [batch_size,seq_len,hidden_dim]\n",
    "        # v: [batch_size,seq_len,hidden_dim]\n",
    "        \n",
    "        q=q.view(batch_size,seq_len,self.head_num,self.head_dim)\n",
    "        k=k.view(batch_size,seq_len,self.head_num,self.head_dim)\n",
    "        v=v.view(batch_size,seq_len,self.head_num,self.head_dim)\n",
    "        print(\"2\")\n",
    "        # print(q.shape)\n",
    "        q, k = apply_rotary_emb(q, k, pos_cis)  # 应用旋转位置编码\n",
    "\n",
    "        q = q.transpose(1, 2)  # 调整 Q 的形状\n",
    "        k = k.transpose(1, 2)  # 调整 K 的形状\n",
    "        v = v.transpose(1, 2)  # 调整 V 的形状\n",
    "        # print(q.shape)\n",
    "\n",
    "\n",
    "        # q: [batch_size,elf.head_num,seq_len,s,self.head_dim]\n",
    "        # k: [batch_size,elf.head_num,seq_len,s,self.head_dim]\n",
    "        # v: [batch_size,elf.head_num,seq_len,s,self.head_dim]\n",
    "\n",
    "        atten_weight=q@k.transpose(-2,-1)/math.sqrt(self.head_dim)\n",
    "        atten_weight+=self.mask[:seq_len,:seq_len]\n",
    "        \n",
    "        atten_weight=torch.softmax(atten_weight,dim=-1)\n",
    "        atten_weight=self.att_dropout(atten_weight)\n",
    "        attention_output=atten_weight@v\n",
    "\n",
    "        attention_output=attention_output.transpose(1,2).contiguous()\n",
    "\n",
    "        attention_output=attention_output.view(batch_size,seq_len,self.hidden_dim)\n",
    "        return self.out_proj(attention_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 512])\n",
      "1\n",
      "2\n",
      "torch.Size([1, 2, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "token = torch.randn(1,2,512)\n",
    "model=MutiHeadattention()\n",
    "params=LMconfig()\n",
    "_bszize, seqlen,_ = token.shape\n",
    "pos_cis=precompute_pos_cis(params.dim // params.head_num, params.max_seq_len)  # 预计算位置编码\n",
    "pos_cis = pos_cis[:seqlen]\n",
    "print(model(token,pos_cis).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
