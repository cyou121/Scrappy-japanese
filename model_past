import numpy as np
import torch
import torch.nn.functional as F
from torch import nn
import math
import CausalLMOutputWithPast


class LMconfig():
    def __init__(self)-> None:
        dim: int = 512,  # 模型维度，默认为 512
        n_layers: int = 8,  # Transformer 层数，默认为 8
        n_heads: int = 16,  # 注意力头数，默认为 16
        n_kv_heads: int = 8,  # KV 头数，默认为 8
        vocab_size: int = 6400,  # 词汇表大小，默认为 6400
        hidden_dim: int = None,  # 隐藏层维度，默认为 None
        multiple_of: int = 64,  # 隐藏层维度的倍数，默认为 64
        norm_eps: float = 1e-5,  # 归一化层的 epsilon 值，默认为 1e-5
        max_seq_len: int = 512,  # 最大序列长度，默认为 512
        dropout: float = 0.0,  # Dropout 概率，默认为 0.0
        flash_attn: bool = True,  # 是否使用 Flash Attention，默认为 True



class RMS(torch.nn.Module):
    def __init__(self, dim: int, eps: float):
        super().__init__()
        self.eps = eps  # 设置 epsilon，防止除零错误
        self.weight = nn.Parameter(torch.ones(dim))  # 初始化权重参数

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)  # 计算 RMSNorm

    def forward(self, x):
        output = self._norm(x.float()).type_as(x)  # 应用 RMSNorm
        return output * self.weight  # 乘以权重参数

class MutiHeadattention(nn.Module):
    def __init__(self,hidden_dim: int=12,head_num: int=3,max_seq_len:int=4)->None:
        super().__init__()
        self.hidden_dim=hidden_dim
        self.head_num=head_num
        self.head_dim=hidden_dim//head_num

        self.q_proj=nn.Linear(self.head_dim,hidden_dim)
        self.k_proj=nn.Linear(self.head_dim,hidden_dim)
        self.v_proj=nn.Linear(self.head_dim,hidden_dim)
        
        self.att_dropout=nn.Dropout(0.1)
        self.out_proj=nn.Linear(hidden_dim,self.head_dim)
        
        mask = torch.full((max_seq_len, max_seq_len), float("-inf"))  
        mask = torch.triu(mask, diagonal=1)  
        self.k_cache=None
        self.v_cache=None
        self.use_kvcache=None
        self.register_buffer("mask", mask)  

    def forward(self,x):
        batch_size,seq_len,_ =x.shape

        if self.use_kvcache and self.eval():
            if self.k_cache and self.v_cache is not None:
                token=x[:,-1,:]
                q=torch.cat(torch.zeros_like(x[:, :-1, :]),self.q_proj(token),dim=1)
                k=torch.cat(self.k_cache,self.k_proj(token),dim=1)
                v=torch.cat(self.v_cache,self.v_proj(token),dim=1)
            else:
                q=self.q_proj(x)
                k=self.k_proj(x)
                v=self.v_proj(x)
            self.k_cache=k
            self.v_cache=v
        else:
            q=self.q_proj(x)
            k=self.k_proj(x)
            v=self.v_proj(x)


        print("1")
        # q: [batch_size,seq_len,hidden_dim]
        # k: [batch_size,seq_len,hidden_dim]
        # v: [batch_size,seq_len,hidden_dim]
        
        q=q.view(batch_size,seq_len,self.head_num,self.head_dim).transpose(1,2)
        k=k.view(batch_size,seq_len,self.head_num,self.head_dim).transpose(1,2)
        v=v.view(batch_size,seq_len,self.head_num,self.head_dim).transpose(1,2)
        print("2")

        # q: [batch_size,elf.head_num,seq_len,s,self.head_dim]
        # k: [batch_size,elf.head_num,seq_len,s,self.head_dim]
        # v: [batch_size,elf.head_num,seq_len,s,self.head_dim]

        atten_weight=q@k.transpose(-2,-1)/math.sqrt(self.head_dim)
        atten_weight+=self.mask[:seq_len,:seq_len]
        
        atten_weight=torch.softmax(atten_weight,dim=-1)
        print(atten_weight)
        atten_weight=self.att_dropout(atten_weight)
        attention_output=atten_weight@v

        attention_output=attention_output.transpose(1,2).contiguous()

        attention_output=attention_output.view(batch_size,seq_len,self.hidden_dim)
        return self.out_proj(attention_output)



class FFN(nn.Module):
    def __init__(self)-> None:
     super().__init__()
     self.Linear=nn.Linear(LMconfig.dim,LMconfig.dim,bias=False)
     self.RMS=RMS()
     self.dropout=nn.Dropout(0.1)


    def forward(self, x):
        start=x
        x=self.RMS(x)
        x1 = self.Linear(x)
        x2 = self.Linear(x)
        x2=F.silu(x2)
        x3=torch.matmul(x1,x2)
        x3=self.Linear(x3)
        x3=self.dropout(x3)
        x3+=start

        return x3


class TransformerBlock(nn.Module):
    def __init__(self,LMconfig)-> None:
     super().__init__()
     self.MutiHeadattention=MutiHeadattention()
     self.FFN=FFN()

    def forward(self, x):
       x1= self.MutiHeadattention(x)+x
       x2= self.FFN(x1)+x1
       return x2
        



class Transformer(nn.Module):
    def __init__(self)-> None:
        super().__init__()
        params=LMconfig()
        self.TransformerBlock=TransformerBlock()
        self.decoder_layers=nn.ModuleList([self.TransformerBlock for i in range(params.n_layers)])
        self.rmsnorm=RMS(params.dim,eps=params.eps)##1

        self.embedding=nn.Embedding(params.vocab_size,params.dim)

        self.Linear=nn.Linear(params.dim,params.vocab_size,bias=False)
        self.OUT = CausalLMOutputWithPast()  

    def forward(self, tokens: torch.Tensor, targets: torch.Tensor,**keyargs) ->torch.Tensor:
        # if 'input_ids' in keyargs:
        #     token=keyargs['input_ids']
        print("tokens:",tokens)
        print("targets:",targets)
        try:
         _bszize, seqlen,_ = tokens.shape
        except ValueError as e:
          print("训练时，输入的Size有错,训练中断")
        
            
        token=nn.Embedding(token)
        for item,decoder_layers in enumerate(self.decoder_layers):
            token=decoder_layers(token)
        token=self.rmsnorm(token)
        logits=self.Linear(token)
        # token=nn.Softmax(token,dim=1)

        print("合并batch 一次性计算交叉熵损失，的矩阵大小为：",logits.view(-1, logits.size(-1)).shape)
        self.last_loss=F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)  # 计算交叉熵损失

        self.OUT.__setitem__('logits', logits)  # 设置输出对象的 logits
        self.OUT.__setitem__('last_loss', self.last_loss)  # 设置输出对象的 last_loss
        

        return self.OUT

    def generate(self,token):

        return token




        


